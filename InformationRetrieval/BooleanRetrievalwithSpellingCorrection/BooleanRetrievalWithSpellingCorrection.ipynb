{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Boolean Retrieval System with Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import total_ordering, reduce\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@total_ordering\n",
    "class Posting:\n",
    "    \n",
    "    def __init__(self, docID):\n",
    "        self._docID = docID\n",
    "        \n",
    "    def get_from_corpus(self, corpus):\n",
    "        return corpus[self._docID]\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self._docID == other._docID\n",
    "    \n",
    "    def __gt__(self, other):\n",
    "        return self._docID > other._docID\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self._docID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posting Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostingList:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._postings = []\n",
    "    \n",
    "    @classmethod\n",
    "    def from_docID(cls, docID):\n",
    "        plist = cls()\n",
    "        plist._postings = [(Posting(docID))]\n",
    "        return plist\n",
    "    \n",
    "    @classmethod\n",
    "    def from_posting_list(cls, postingList):\n",
    "        plist = cls()\n",
    "        plist._postings = postingList\n",
    "        return plist\n",
    "    \n",
    "    def merge(self, other):\n",
    "        i = 0\n",
    "        last = self._postings[-1]\n",
    "        while (i < len(other._postings) and last == other._postings[i]):\n",
    "            i += 1\n",
    "        self._postings += other._postings[i:]\n",
    "        \n",
    "    def intersection(self, other):\n",
    "        intersection = []\n",
    "        i = 0\n",
    "        j = 0\n",
    "        while (i < len(self._postings) and j < len(other._postings)):\n",
    "            if (self._postings[i] == other._postings[j]):\n",
    "                intersection.append(self._postings[i])\n",
    "                i += 1\n",
    "                j += 1\n",
    "            elif (self._postings[i] < other._postings[j]):\n",
    "                i += 1\n",
    "            else:\n",
    "                j += 1\n",
    "        return PostingList.from_posting_list(intersection)\n",
    "    \n",
    "    def union(self, other):\n",
    "        union = []\n",
    "        i = 0\n",
    "        j = 0\n",
    "        while (i < len(self._postings) and j < len(other.postings)):\n",
    "            if (self._postings[i] == other._postings[j]):\n",
    "                union.append(self._postings[i])\n",
    "                i += 1\n",
    "                j += 1\n",
    "            elif (self._postings[i] < other._postings[j]):\n",
    "                union.append(self._postings[i])\n",
    "                i += 1\n",
    "            else:\n",
    "                union.append(other._postings[j])\n",
    "                j += 1\n",
    "        for k in range(i, len(self._postings)):\n",
    "            union.append(self._postings[k])\n",
    "        for k in range(j, len(other._postings)):\n",
    "            union.append(other._postings[k])\n",
    "        return PostingList.from_posting_list(union)\n",
    "    \n",
    "    def get_from_corpus(self, corpus):\n",
    "        return list(map(lambda x: x.get_from_corpus(corpus), self._postings))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \", \".join(map(str, self._postings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImpossibleMergeError(Exception):\n",
    "    pass\n",
    "\n",
    "@total_ordering\n",
    "class Term:\n",
    "    \n",
    "    def __init__(self, term, docID):\n",
    "        self.term = term\n",
    "        self.posting_list = PostingList.from_docID(docID)\n",
    "        \n",
    "    def merge(self, other):\n",
    "        if (self.term == other.term):\n",
    "            self.posting_list.merge(other.posting_list)\n",
    "        else:\n",
    "            raise ImpossibleMergeError\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.term == other.term\n",
    "    \n",
    "    def __gt__(self, other):\n",
    "        return self.term > other.term\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.term + \": \" + repr(self.posting_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    no_punctuation = re.sub(r'[^\\w^\\s^-]','',text)\n",
    "    downcase = no_punctuation.lower()\n",
    "    return downcase\n",
    "\n",
    "def tokenize(movie):\n",
    "    text = normalize(movie.description)\n",
    "    return list(text.split())\n",
    "\n",
    "class InvertedIndex:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._dictionary = []\n",
    "        \n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus):\n",
    "        intermediate_dict = {}\n",
    "        for docID, document in enumerate(corpus):\n",
    "            tokens = tokenize(document)\n",
    "            for token in tokens:\n",
    "                term = Term(token, docID)\n",
    "                try:\n",
    "                    intermediate_dict[token].merge(term)\n",
    "                except KeyError:\n",
    "                    intermediate_dict[token] = term\n",
    "            if (docID % 1000 == 0):\n",
    "                print(\"ID: \" + str(docID))\n",
    "        idx = cls()\n",
    "        idx._dictionary = sorted(intermediate_dict.values())\n",
    "        return idx\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        for term in self._dictionary:\n",
    "            if term.term == key:\n",
    "                return term.posting_list\n",
    "        raise KeyError\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"A dictionary with \" + str(len(self._dictionary)) + \" terms\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieDescription:\n",
    "    \n",
    "    def __init__(self, title, description):\n",
    "        self.title = title\n",
    "        self.description = description\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.title\n",
    "    \n",
    "def read_movie_descriptions():\n",
    "    filename = 'plot_summaries.txt'\n",
    "    movie_names_file = 'movie.metadata.tsv'\n",
    "    with open(movie_names_file, 'r') as csv_file:\n",
    "        movie_names = csv.reader(csv_file, delimiter='\\t')\n",
    "        names_table = {}\n",
    "        for name in movie_names:\n",
    "            names_table[name[0]] = name[2]\n",
    "    with open(filename, 'r') as csv_file:\n",
    "        descriptions = csv.reader(csv_file, delimiter='\\t')\n",
    "        corpus = []\n",
    "        for desc in descriptions:\n",
    "            try:\n",
    "                movie = MovieDescription(names_table[desc[0]], desc[1])\n",
    "                corpus.append(movie)\n",
    "            except KeyError:\n",
    "                pass\n",
    "        return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRsystem:\n",
    "    \n",
    "    def __init__(self, corpus, index):\n",
    "        self._corpus = corpus\n",
    "        self._index = index\n",
    "        \n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus):\n",
    "        index = InvertedIndex.from_corpus(corpus)\n",
    "        return cls(corpus, index)\n",
    "    \n",
    "    def answer_query(self, words): # ['cat', 'batman']\n",
    "        norm_words = map(normalize, words)\n",
    "        postings = map(lambda w: self._index[w], norm_words)\n",
    "        plist = reduce(lambda x, y: x.intersection(y), postings)\n",
    "        return plist.get_from_corpus(self._corpus)\n",
    "    \n",
    "    # New methods, this time with spelling correction!\n",
    "    def answer_query_sc(self, words):\n",
    "        norm_words = map(normalize, words)\n",
    "        postings = []\n",
    "        for w in norm_words:\n",
    "            try:\n",
    "                res = self._index[w]\n",
    "            except KeyError:\n",
    "                dictionary = [t.term for t in self._index._dictionary]\n",
    "                sub = find_nearest(w, dictionary, keep_first=True)\n",
    "                print(\"{} not found. Did you mean {}?\".format(w, sub))\n",
    "                res = self._index[sub]\n",
    "            postings.append(res)\n",
    "        plist = reduce(lambda x, y: x.intersection(y), postings)\n",
    "        return plist.get_from_corpus(self._corpus)\n",
    "    \n",
    "def query(ir, text):\n",
    "    words = text.split()\n",
    "    answer = ir.answer_query(words)\n",
    "    for movie in answer:\n",
    "        print(movie)\n",
    "        \n",
    "def query_sc(ir, text):\n",
    "    words = text.split()\n",
    "    answer = ir.answer_query_sc(words)\n",
    "    for movie in answer:\n",
    "        print(movie)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_distance(u, v):\n",
    "    nrows = len(u) + 1\n",
    "    ncols = len(v) + 1\n",
    "    M = [[0] * ncols for i in range(0, nrows)]\n",
    "    for i in range(0, nrows):\n",
    "        M[i][0] = i\n",
    "    for j in range(0, ncols):\n",
    "        M[0][j] = j\n",
    "    for i in range(1, nrows):\n",
    "        for j in range(1, ncols):\n",
    "            candidates = [M[i-1][j] + 1, M[i][j-1] + 1]\n",
    "            if (u[i-1] == v[j-1]):\n",
    "                candidates.append(M[i-1][j-1])\n",
    "            else:\n",
    "                candidates.append(M[i-1][j-1] + 1)\n",
    "            M[i][j] = min(candidates)\n",
    "    return M[-1][-1]\n",
    "\n",
    "def find_nearest(word, dictionary, keep_first=False):\n",
    "    if keep_first:\n",
    "        dictionary = [w for w in dictionary if w[0] == word[0]]\n",
    "    distances = map(lambda x: edit_distance(word, x), dictionary)\n",
    "    # [(45, \"aaa\"), ...] \n",
    "    return min(zip(distances, dictionary))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 0\n",
      "ID: 1000\n",
      "ID: 2000\n",
      "ID: 3000\n",
      "ID: 4000\n",
      "ID: 5000\n",
      "ID: 6000\n",
      "ID: 7000\n",
      "ID: 8000\n",
      "ID: 9000\n",
      "ID: 10000\n",
      "ID: 11000\n",
      "ID: 12000\n",
      "ID: 13000\n",
      "ID: 14000\n",
      "ID: 15000\n",
      "ID: 16000\n",
      "ID: 17000\n",
      "ID: 18000\n",
      "ID: 19000\n",
      "ID: 20000\n",
      "ID: 21000\n",
      "ID: 22000\n",
      "ID: 23000\n",
      "ID: 24000\n",
      "ID: 25000\n",
      "ID: 26000\n",
      "ID: 27000\n",
      "ID: 28000\n",
      "ID: 29000\n",
      "ID: 30000\n",
      "ID: 31000\n",
      "ID: 32000\n",
      "ID: 33000\n",
      "ID: 34000\n",
      "ID: 35000\n",
      "ID: 36000\n",
      "ID: 37000\n",
      "ID: 38000\n",
      "ID: 39000\n",
      "ID: 40000\n",
      "ID: 41000\n",
      "ID: 42000\n"
     ]
    }
   ],
   "source": [
    "corpus = read_movie_descriptions()\n",
    "ir = IRsystem.from_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yioda not found. Did you mean yoda?\n",
      "lukke not found. Did you mean luke?\n",
      "darhth not found. Did you mean darth?\n",
      "Star Wars Episode V: The Empire Strikes Back\n",
      "Something, Something, Something Dark Side\n",
      "Return of the Ewok\n",
      "Star Wars Episode III: Revenge of the Sith\n",
      "Star Wars Episode VI: Return of the Jedi\n",
      "It's a Trap!\n"
     ]
    }
   ],
   "source": [
    "query_sc(ir, \"yioda lukke darhth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
